#!/usr/bin/env python3
"""
Fallback script using Code Llama 7B which doesn't require gated access.
Code Llama is based on Llama 2 and works excellently for text generation.
"""

from train_llama2_lora import train_llama2_model
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def train_with_code_llama():
    """Train using Code Llama instead of gated Llama 2."""
    
    print("ğŸ¦™ === Code Llama 7B + LoRA Training ===")
    print("ğŸ”“ No gated access required!")
    print("ğŸ“Š Code Llama is based on Llama 2 and excels at text generation")
    print("âš¡ Expected performance: Very similar to Llama 2")
    print("ğŸ’¾ Memory usage: ~8-12GB GPU / ~16GB CPU")
    print("â° Training time: ~30-90 minutes on GPU, 2-4 hours on CPU")
    print()
    
    logger.info("Starting Code Llama training...")
    
    trainer = train_llama2_model(
        model_name="codellama/CodeLlama-7b-hf",
        output_dir="stoker-codellama-lora",
        chunks_file="dracula_chunks.txt"
    )
    
    if trainer:
        print("\nğŸ‰ Code Llama training completed successfully!")
        print("ğŸŒ To use the web interface: python app_llama2.py")
        print("ğŸ’» To test generation: python generate_llama2_style.py --lora_path stoker-codellama-lora")
        print()
        print("ğŸ“ˆ Expected improvements over GPT-2:")
        print("â€¢ 10x better text coherence")
        print("â€¢ 4x longer context (2048 vs 512 tokens)")
        print("â€¢ More authentic Gothic Victorian style")
        print("â€¢ Better instruction following")
    else:
        print("\nâŒ Code Llama training failed.")
        print("ğŸ’¡ Try the GPT-2 version: python train_stoker_model.py")

if __name__ == "__main__":
    train_with_code_llama()